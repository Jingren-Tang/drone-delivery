{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb37443",
   "metadata": {},
   "source": [
    "# Car-following model (Latent class basic specification)\n",
    "\n",
    "This notebook was written by Evangelos Paschalidis (evangelos.paschalidis@epfl.ch) for the Decision-aid methodologies in transportation course at EPFL (http://edu.epfl.ch/coursebook/en/decision-aid-methodologies-in-transportation-CIVIL-557). \n",
    "\n",
    "Please contact before distributing or reusing the material below.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers the estimation of a GM car-following model in python with maximum likelihhod estimation:\n",
    "\n",
    "* Load necessary packages\n",
    "* Define variables and parameters to estimate\n",
    "* Model specification\n",
    "* Model output\n",
    "\n",
    "Have a go at working through the notebook. To run a code cell, just click on it (to see a green box around it) and then press the **Run** button at the top! \n",
    "\n",
    "Some cells have blank lines for you to complete. There is always a comment telling you what to do!\n",
    "\n",
    "You can also add a new cell by pressing the **+** button at the top of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ad16e",
   "metadata": {},
   "source": [
    "## Load packages\n",
    "\n",
    "Before we estimate the model let's load some packages that we are going to need. When importing a package, it is common to rename it using an abbreviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data frame processing\n",
    "import numpy as np # for some statistical procedures\n",
    "from scipy.optimize import minimize # optimisation routine for parameter estimation\n",
    "from scipy.stats import norm # normal distribution density function\n",
    "import numdifftools as nd # we use this to calculate t-ratios and p-values\n",
    "import csv # we need this to store our parameters as csv\n",
    "\n",
    "# We've got some rookies in the mix.\n",
    "from scipy.special import roots_legendre # we use this to generate nodes for numerical integration\n",
    "from scipy.stats import qmc # we use this to generate nodes for numerical integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431fb5b4",
   "metadata": {},
   "source": [
    "### Let's give a name to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6900880",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'car_following_model_latent_class' # Name we want to give to our model (this is used when saving the output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1791d",
   "metadata": {},
   "source": [
    "### Panel structure\n",
    "We need to define whether our data is panel (i.e. multiple observations per individual) or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = 1 # switch to 1 if data is panel (any other value if not panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c20c41",
   "metadata": {},
   "source": [
    "### Define if we use mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e417dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixing = 0 # switch to 1 if we apply mixing (any other value if no mixing applied)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12a2a2",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Now it is time to load the data. We can do that using the piece of code below.\n",
    "\n",
    "**Important!** Make sure the data is in the same folder with the notebook file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to load the data\n",
    "data = pd.read_table('I80_data0.txt')\n",
    "\n",
    "# Number of observations (we need this to caclulate goodness-of-fit indices)\n",
    "Nobs = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e03b17",
   "metadata": {},
   "source": [
    "## Print the data\n",
    "\n",
    "Let's quickly print the data. Simply type *data* in the field below\n",
    "\n",
    "(If we had opened our data set with a different name e.g. *database*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b4f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Type \"data\" in this field (without the quotation) and run the cell (Shift + return)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcff136",
   "metadata": {},
   "source": [
    "## Print the variable names (columns)\n",
    "\n",
    "We can also print the variable names only using the piece of code below\n",
    "\n",
    "* This is useful during model specification to easily access the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e385d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d70fd3",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "* It may be the case that our data do not include all variables required for modelling.\n",
    "* In this case we must generate the additional variables that we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5837b",
   "metadata": {},
   "source": [
    "### Variable creation\n",
    "\n",
    "* Let's create a \"running task\" variable, else a counter for the observations of each driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['running_task'] = data.groupby(['ID']).cumcount()+1 # counter of observations per individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32bad5f",
   "metadata": {},
   "source": [
    "## Variable definition\n",
    "\n",
    "We need to define the variables (as numpy arrays) that we will use in our model.\n",
    "\n",
    "* The arrays can have any name but it is more convenient to use the same name as in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# Variable_name = np.array(data['Variable_name']).reshape(-1, 1)\n",
    "\n",
    "running_task = np.array(data['running_task']).reshape(-1, 1)\n",
    "Acceleration = np.array(data['Acceleration']).reshape(-1, 1)\n",
    "Speed = np.array(data['Speed']).reshape(-1, 1)\n",
    "Space_headway = np.array(data['Space_headway']).reshape(-1, 1)\n",
    "Speed_lead = np.array(data['Speed_lead']).reshape(-1, 1)\n",
    "Acceleration_lead = np.array(data['Acceleration_lead']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a0529",
   "metadata": {},
   "source": [
    "#### Define the \"lagged\" speed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagged speed values #\n",
    "lag_speed1 = np.array(data['lag_speed1']).reshape(-1, 1)\n",
    "lag_speed2 = np.array(data['lag_speed2']).reshape(-1, 1)\n",
    "lag_speed3 = np.array(data['lag_speed3']).reshape(-1, 1)\n",
    "\n",
    "# Lagged speed lead values #\n",
    "lag_speed_lead1 = np.array(data['lag_speed_lead1']).reshape(-1, 1)\n",
    "lag_speed_lead2 = np.array(data['lag_speed_lead2']).reshape(-1, 1)\n",
    "lag_speed_lead3 = np.array(data['lag_speed_lead3']).reshape(-1, 1)\n",
    "\n",
    "# Lagged acceleration values #\n",
    "lag_acceleration1 = np.array(data['lag_acceleration1']).reshape(-1, 1)\n",
    "lag_acceleration2 = np.array(data['lag_acceleration2']).reshape(-1, 1)\n",
    "lag_acceleration3 = np.array(data['lag_acceleration3']).reshape(-1, 1)\n",
    "\n",
    "# Lagged speed lead values #\n",
    "lag_acceleration_lead1 = np.array(data['lag_acceleration_lead1']).reshape(-1, 1)\n",
    "lag_acceleration_lead2 = np.array(data['lag_acceleration_lead2']).reshape(-1, 1)\n",
    "lag_acceleration_lead3 = np.array(data['lag_acceleration_lead3']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50faec6a",
   "metadata": {},
   "source": [
    "#### Define the ID variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bdaa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = np.array(data['ID']) # ID does not need to be reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58da27f",
   "metadata": {},
   "source": [
    "## Model specification\n",
    "\n",
    "We now need to create a function that includes our model.\n",
    "\n",
    "* Python functions are defined as: def *function_name*(parameters):\n",
    "* We end a function as: return *value_to_return*\n",
    "\n",
    "In the current implementation we specify two different functions as:\n",
    "* *function 1* calculates the log likelihood per observations\n",
    "* *function 2* calculates the sum of log likelihood taking as input the result from *function 1*\n",
    "\n",
    "*We define two different functions to be more flexible in the post estimation processing later in the code*\n",
    "\n",
    "We use some python (numpy) functions such '*exp*' or '*log*'. To execute these in the current example, we need to also call numpy; hence, we have *np.exp()* and *np.log()*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4239e",
   "metadata": {},
   "source": [
    "### Define parameters and starting values\n",
    "\n",
    "Ultimately, we want to estimate the value of some parameters that maximise the likelihood of our observations of the dependent variable.\n",
    "\n",
    "Before starting the estimation process, we need to define some starting values for the parameters to be estimated.\n",
    "\n",
    "* The starting values are usually zeroes\n",
    "* When a parameter is included as a denominator, the starting value cannot be 0 for computational reasons.\n",
    "* However, since we estimate the log of sigma, our starting value can be zero since exp(sigma) can never be absolute zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc72de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_start = {\n",
    "               \"bc_acc\":0, # Constant of acceleration utility\n",
    "               \"bc_dec\":0, # Constant of deceleration utility\n",
    "                \n",
    "                \"b_speed_c1\":0, # Relative speed for acceleration utility\n",
    "                \"b_speed_c2\":0, # Relative speed for deceleration utility\n",
    "                \n",
    "               \"alpha_acc\":0, # Acceleration constant parameter\n",
    "               \"alpha_dec\":0, # Deceleration constant parameter\n",
    "               \"beta_acc\":0,  # Speed (acceleration regime) parameter\n",
    "               \"beta_dec\":0,  # Speed (deceleration regime) parameter\n",
    "                \"gamma_acc\":0, # Space headway (acceleration regime) parameter\n",
    "                \"gamma_dec\":0, # Space headway (deceleration regime) parameter\n",
    "               \"lamda_acc_p\":0, # Relative speed (acceleration regime) parameter\n",
    "               # \"lamda_acc_n\":0, # Relative speed (deceleration regime) parameter\n",
    "               # \"lamda_dec_p\":0, # Relative speed (acceleration regime) parameter\n",
    "               \"lamda_dec_n\":0, # Relative speed (deceleration regime) parameter\n",
    "                \"dn\":0,\n",
    "               \"sigma_acc\":-0, # Std deviation (acceleration regime) parameter\n",
    "               \"sigma_dec\":-0,  # Std deviation (deceleration regime) parameter\n",
    "                \"sigma_dn\":-0  # Std deviation (deceleration regime) parameter\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db5a9c",
   "metadata": {},
   "source": [
    "### Load old parameter estimates results\n",
    "\n",
    "Sometimes, we want to use results from old models as starting values.\n",
    "* To do that, we will load the iteration file from a previous estimation\n",
    "* Please note that only values of parameters with same names with our current model will be copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Activate this cell to load old results ###\n",
    "\n",
    "# # Open old iteration file\n",
    "# betas_old = pd.read_csv('model_name_iterations.csv')\n",
    "\n",
    "# # Keep last row\n",
    "# betas_old = betas_old.tail(1)\n",
    "\n",
    "# # Convert to dictionary\n",
    "# betas_old = betas_old.to_dict(orient='records')[0]\n",
    "\n",
    "# # Copy values from old to start for keys that are common to both dictionaries\n",
    "# betas_start = {k: betas_old[k] for k in betas_start.keys() & betas_old.keys()}\n",
    "\n",
    "# # Delete old estimates\n",
    "# del[betas_old]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeba3aa",
   "metadata": {},
   "source": [
    "### Generation of random draws\n",
    "In this piece of code below, we generate the random draws to be used as reaction time.\n",
    "\n",
    "Two pieces of code are provided one for numerical integration and one for simulation\n",
    "\n",
    "**Only one of the two blocks can be activated at a time!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First let's set the number of draws\n",
    "# # This indicates the number of random draws per random effect\n",
    "# Ndraws = 1\n",
    "\n",
    "# # And then let's set the reaction time range\n",
    "# tau_max = 3 # Max bound of reaction time\n",
    "# tau_min = 0 # Min bound of reaction time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499992f7",
   "metadata": {},
   "source": [
    "#### Method 1: Code for numerical integration\n",
    "With numerical integration we directly get reaction time values in the (0, 3) range (or any other range we set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced87be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This piece of code is generating the nodes and weights using the number of nodes we set just above\n",
    "# roots, weights = roots_legendre(Ndraws) # generate (-1, 1) nodes\n",
    "\n",
    "# tau_max = 3 # Max bound of reaction time\n",
    "# tau_min = 0 # Min bound of reaction time\n",
    "# tau = 0.5*(tau_max-tau_min)*roots+0.5*(tau_max+tau_min) # convert nodes (-1, 1) to reaction time values (0, 3)\n",
    "\n",
    "# # Some data manipulation to correct the reaction time matix dimensions\n",
    "# tau = np.transpose(tau)\n",
    "# tau = np.tile(tau, (len(data), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906bf0a",
   "metadata": {},
   "source": [
    "#### Method 2: Code for simulation\n",
    "The simulation code will generate uniform random numbers (0, 1).\n",
    "\n",
    "These numbers must be then used in the model function (Function 1) to convert them to reaction time values using truncated distribution simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This piece of code is generating Halton draws using the number of nodes we set just above\n",
    "# nIndiv = len(set(ID))\n",
    "# draws = ['tau_draws']\n",
    "# nDim = len(draws)\n",
    "# tasks = (pd.DataFrame(ID).value_counts(sort = False)) # observations per ID\n",
    "\n",
    "# sampler = qmc.Halton(d=nDim, scramble=False)\n",
    "# sample = pd.DataFrame(sampler.random(n=Ndraws*nIndiv+Ndraws)) # +Ndraws\n",
    "# sample = sample[(Ndraws-1):(Ndraws*nIndiv+Ndraws-1)]\n",
    "\n",
    "# cols = len(sample.columns)\n",
    "\n",
    "# for i in range(cols):\n",
    "#     # print(i)\n",
    "#     sample1 = np.array(sample.loc[:,i])\n",
    "#     sample1=pd.DataFrame(np.reshape(sample1,(nIndiv,Ndraws)))\n",
    "#     sample_rep =sample1.loc[sample1.index.repeat(tasks)]\n",
    "#     globals()[draws[i]] = sample_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to normal distribution (or any other preferred)\n",
    "# Here we use some funciton to convert the uniform draws we obtained\n",
    "# in the previous block of code to any distirbution\n",
    "# Example: draws_variable = norm.ppf(draws_variable)\n",
    "# The norm.ppf() command converts the uniform draws to standard normal\n",
    "\n",
    "# NOTE: This piece of code is not relevant process for the car-following model\n",
    "# We do this distribution transformation process within Function 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138ce6b",
   "metadata": {},
   "source": [
    "#### Calculate lagged speed values based on the reaction time values\n",
    "\n",
    "This step is **Only** relevant for numerical integration.\n",
    "\n",
    "If simulation is used then the block of code below must be pasted inside Function 1 and not before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################################################################################################\n",
    "# # Here we calculate the lagged relative speed given the reaction time values we generated above #\n",
    "# #################################################################################################\n",
    "\n",
    "# # The way this is done is by implementing linear interpolation between the observations around each reaction time value\n",
    "# # We simply assume that \"acceleration = (speed1 - speed0) / (time1 - time0)\"\n",
    "# # We take the observation closest to the reaction time value\n",
    "\n",
    "# # We calculate lag speed\n",
    "# lag_speed = ((np.intc(tau)==0) * ((np.intc(tau)+1-tau<0.5)*(lag_speed1+lag_acceleration1*(np.intc(tau)+1-tau))+\n",
    "#                                     (np.intc(tau)+1-tau>=0.5)*(Speed-Acceleration*(tau-np.intc(tau))))+\n",
    "#              (np.intc(tau)==1) * ((np.intc(tau)+1-tau<0.5)*(lag_speed2+lag_acceleration2*(np.intc(tau)+1-tau))+\n",
    "#                             (np.intc(tau)+1-tau>=0.5)*(lag_speed1-lag_acceleration1*(tau-np.intc(tau))))+\n",
    "#              (np.intc(tau)==2) * ((np.intc(tau)+1-tau<0.5)*(lag_speed3+lag_acceleration3*(np.intc(tau)+1-tau))+\n",
    "#                             (np.intc(tau)+1-tau>=0.5)*(lag_speed2-lag_acceleration2*(tau-np.intc(tau)))))\n",
    "\n",
    "# # We correct lag speed if the result is negative\n",
    "# lag_speed = (lag_speed*(lag_speed>=0))+((np.round(tau)==0)*Speed+(np.round(tau)==1)*lag_speed1+(np.round(tau)==2)*lag_speed2+(np.round(tau)==3)*lag_speed3)*(lag_speed<0)\n",
    "\n",
    "# # We calculate lag speed of leader\n",
    "# lag_speed_lead = ((np.intc(tau)==0) * ((np.intc(tau)+1-tau<0.5)*(lag_speed_lead1+lag_acceleration_lead1*(np.intc(tau)+1-tau))+(np.intc(tau)+1-tau>=0.5)*(Speed_lead-Acceleration_lead*(tau-np.intc(tau))))+\n",
    "#                   (np.intc(tau)==1) * ((np.intc(tau)+1-tau<0.5)*(lag_speed_lead2+lag_acceleration_lead2*(np.intc(tau)+1-tau))+(np.intc(tau)+1-tau>=0.5)*(lag_speed_lead1-lag_acceleration_lead1*(tau-np.intc(tau))))+\n",
    "#                   (np.intc(tau)==2) * ((np.intc(tau)+1-tau<0.5)*(lag_speed_lead3+lag_acceleration_lead3*(np.intc(tau)+1-tau))+(np.intc(tau)+1-tau>=0.5)*(lag_speed_lead2-lag_acceleration_lead2*(tau-np.intc(tau)))))\n",
    "\n",
    "# # We correct the lag speed of leader if the result is negative\n",
    "# lag_speed_lead = (lag_speed_lead*(lag_speed_lead>=0))+((np.round(tau)==0)*Speed_lead+(np.round(tau)==1)*lag_speed_lead1+(np.round(tau)==2)*lag_speed_lead2+(np.round(tau)==3)*lag_speed_lead3)*(lag_speed_lead<0)\n",
    "\n",
    "\n",
    "# # We calculate lagged relative speed as the difference\n",
    "# Lagged_relative_speed = lag_speed_lead-lag_speed # Lagged relative speed per reaction time value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca7ff2",
   "metadata": {},
   "source": [
    "### Function 1: log likelihood (LL)\n",
    "This function calculates the log likelihood per observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LL(betas): # betas is a vector with the parameters we want to estimate\n",
    "   \n",
    "    # First let's define the parameters to be estimated.\n",
    "    # The parameter names are imported directly from 'beta_start' that we defined earlier\n",
    "    \n",
    "    for pn in range(len(betas_start.values())):\n",
    "        globals()[np.array(list(betas_start.keys()))[pn]] = betas[pn]\n",
    "\n",
    "    ############################################################################################################    \n",
    "    ############################################################################################################    \n",
    "    # Calculation of reaction time values\n",
    "    # This part of the code must ONLY be activated if simulation is used\n",
    "    # Below we convert the uniofrm numbers (tau_draws) to truncated log-normal\n",
    "    \n",
    "#     tau = np.exp(norm.ppf(\n",
    "#        tau_draws*(norm.cdf(np.log(tau_max),tau_mean,np.exp(sigma_tau))) # -norm.cdf(np.log(tau_min),tau_mean,np.exp(sigma_tau))\n",
    "#        )*np.exp(sigma_tau)+tau_mean)\n",
    "    \n",
    "    # #################################################################################################\n",
    "    # # Here we calculate the lagged relative speed given the reaction time values we generated above #\n",
    "    # #################################################################################################\n",
    "\n",
    "    # # The way this is done is by implementing linear interpolation between the observations around each reaction time value\n",
    "    # # We simply assume that \"acceleration = (speed1 - speed0) / (time1 - time0)\"\n",
    "    # # We take the observation closest to the reaction time value\n",
    "    \n",
    "    # We calculate lag speed\n",
    "#     lag_speed = ((np.intc(tau)==0) * ((np.intc(tau)+1-tau<0.5)*(lag_speed1+lag_acceleration1*(np.intc(tau)+1-tau))+\n",
    "#                                         (np.intc(tau)+1-tau>=0.5)*(Speed-Acceleration*(tau-np.intc(tau))))+\n",
    "#                  (np.intc(tau)==1) * ((np.intc(tau)+1-tau<0.5)*(lag_speed2+lag_acceleration2*(np.intc(tau)+1-tau))+\n",
    "#                                 (np.intc(tau)+1-tau>=0.5)*(lag_speed1-lag_acceleration1*(tau-np.intc(tau))))+\n",
    "#                  (np.intc(tau)==2) * ((np.intc(tau)+1-tau<0.5)*(lag_speed3+lag_acceleration3*(np.intc(tau)+1-tau))+\n",
    "#                                 (np.intc(tau)+1-tau>=0.5)*(lag_speed2-lag_acceleration2*(tau-np.intc(tau)))))\n",
    "\n",
    "#     # We correct lag speed if the result is negative\n",
    "#     lag_speed = (lag_speed*(lag_speed>=0))+((np.round(tau)==0)*Speed+(np.round(tau)==1)*lag_speed1+(np.round(tau)==2)*lag_speed2+(np.round(tau)==3)*lag_speed3)*(lag_speed<0)\n",
    "\n",
    "#     # We calculate lag speed of leader\n",
    "#     lag_speed_lead = ((np.intc(tau)==0) * ((np.intc(tau)+1-tau<0.5)*(lag_speed_lead1+lag_acceleration_lead1*(np.intc(tau)+1-tau))+(np.intc(tau)+1-tau>=0.5)*(Speed_lead-Acceleration_lead*(tau-np.intc(tau))))+\n",
    "#                       (np.intc(tau)==1) * ((np.intc(tau)+1-tau<0.5)*(lag_speed_lead2+lag_acceleration_lead2*(np.intc(tau)+1-tau))+(np.intc(tau)+1-tau>=0.5)*(lag_speed_lead1-lag_acceleration_lead1*(tau-np.intc(tau))))+\n",
    "#                       (np.intc(tau)==2) * ((np.intc(tau)+1-tau<0.5)*(lag_speed_lead3+lag_acceleration_lead3*(np.intc(tau)+1-tau))+(np.intc(tau)+1-tau>=0.5)*(lag_speed_lead2-lag_acceleration_lead2*(tau-np.intc(tau)))))\n",
    "\n",
    "#     # We correct the lag speed of leader if the result is negative\n",
    "#     lag_speed_lead = (lag_speed_lead*(lag_speed_lead>=0))+((np.round(tau)==0)*Speed_lead+(np.round(tau)==1)*lag_speed_lead1+(np.round(tau)==2)*lag_speed_lead2+(np.round(tau)==3)*lag_speed_lead3)*(lag_speed_lead<0)\n",
    "\n",
    "\n",
    "#     # We calculate lagged relative speed as the difference\n",
    "#     Lagged_relative_speed = lag_speed_lead-lag_speed # Lagged relative speed per reaction time value\n",
    "    \n",
    "    Lagged_relative_speed = lag_speed_lead1-lag_speed1\n",
    "    # Lagged_relative_speed = Speed_lead-Speed\n",
    "    \n",
    "    \n",
    "    ############################################################################################################\n",
    "    ############################################################################################################\n",
    "    \n",
    "    # Latent classes\n",
    "    \n",
    "    # Utility functions\n",
    "    U_acc = bc_acc + b_speed_c1*Lagged_relative_speed*(Lagged_relative_speed>0)\n",
    "    U_dec = bc_dec + b_speed_c2*Lagged_relative_speed*(Lagged_relative_speed<0)\n",
    "    U_dn = 0\n",
    "\n",
    "    # Utility exponentials\n",
    "    U_acc = np.exp(U_acc)\n",
    "    U_dec = np.exp(U_dec)\n",
    "    U_dn = np.exp(U_dn)\n",
    "    \n",
    "    # Sum of utilities\n",
    "    sum_all = U_acc + U_dec + U_dn\n",
    "    \n",
    "    # Probabilities\n",
    "    P_acc = U_acc/sum_all\n",
    "    P_dec = U_dec/sum_all\n",
    "    P_dn = U_dn/sum_all\n",
    "    \n",
    "    ############################################################################################################\n",
    "    ############################################################################################################\n",
    "    \n",
    "    # Car-following model specification\n",
    "\n",
    "    # BE CAREFUL!! Below we add a small correction term (np.exp(-50)) to speed and relative speed.\n",
    "    # This correction is included to avoid estimation issues if the value of the independent variables is 0.\n",
    "    \n",
    "    # BE CAREFUL!! We force the signs of alpha_acc and alpha_dec by using then inside an exp() funciton.\n",
    "    \n",
    "    # Sensitivity term\n",
    "    sensitivity_acc = np.exp(alpha_acc)*((Speed+np.exp(-50))**beta_acc)/(Space_headway**gamma_acc)\n",
    "    sensitivity_dec = -np.exp(alpha_dec)*((Speed+np.exp(-50))**beta_dec)/(Space_headway**gamma_dec)\n",
    "\n",
    "    # Stimulus term (we add a condition to keep only the positive values for acceleration and negative for deceleration)\n",
    "    stimulus_acc = (\n",
    "        (np.abs(Lagged_relative_speed + np.exp(-50))**lamda_acc_p)**(Lagged_relative_speed>=0)\n",
    "        # (np.abs(Lagged_relative_speed + np.exp(-50))**lamda_acc_n)*(Lagged_relative_speed<0)\n",
    "        )\n",
    "    \n",
    "    stimulus_dec = (\n",
    "        # (np.abs(Lagged_relative_speed + np.exp(-50))**lamda_dec_p)*(Lagged_relative_speed>=0)+\n",
    "        (np.abs(Lagged_relative_speed + np.exp(-50))**lamda_dec_n)**(Lagged_relative_speed<0)\n",
    "        )\n",
    "\n",
    "    # Acceleration - deceleration means\n",
    "    acc = sensitivity_acc*stimulus_acc\n",
    "    dec = sensitivity_dec*stimulus_dec\n",
    "\n",
    "    # Density functions for acceleration and deceleration\n",
    "    pdf_acc = norm.pdf(Acceleration,acc,np.exp(sigma_acc))/(1-norm.cdf(0,acc,np.exp(sigma_acc)))\n",
    "    pdf_dec = norm.pdf(Acceleration,dec,np.exp(sigma_dec))/(norm.cdf(0,dec,np.exp(sigma_dec)))\n",
    "    \n",
    "    pdf_dn = norm.pdf((Acceleration),dn,np.exp(sigma_dn))\n",
    "    \n",
    "    # Combined probability of acceleration and deceleration regimes\n",
    "    P = ((pdf_acc*P_acc + pdf_dn*P_dn)*(Acceleration>=0)+\n",
    "         (pdf_dec*P_dec + pdf_dn*P_dn)*(Acceleration<0)\n",
    "         )\n",
    "    \n",
    "    \n",
    "    ############################################################################################################\n",
    "    # - Now this below is relevant if we have panel data and apply mixing (Do not change this piece of code!) -#\n",
    "    if panel == 1:\n",
    "    # Do it as panel\n",
    "        P = pd.DataFrame(P)\n",
    "        P = pd.concat([pd.Series(ID), pd.DataFrame(P)], axis=1, ignore_index=True)\n",
    "        P.rename(columns={P.columns[0]: 'ID'},inplace=True)\n",
    "    \n",
    "        # We take the product of probabilities per individual per draw and then delete the ID column\n",
    "        P = P.groupby('ID', as_index=False).prod()\n",
    "        P = P.drop('ID', axis=1)\n",
    "   \n",
    "    if mixing == 1:\n",
    "        # We take the average per row to get the average probability per individual (if mixing == 1)\n",
    "        \n",
    "        if panel == 1:\n",
    "            P['mean'] = P.mean(axis=1)\n",
    "            P = np.array(P['mean'])\n",
    "        \n",
    "        if panel == 0:\n",
    "            P = pd.DataFrame(P)\n",
    "            P = pd.concat([pd.Series(ID), pd.DataFrame(P)], axis=1, ignore_index=True)\n",
    "            P.rename(columns={P.columns[0]: 'ID'},inplace=True)\n",
    "    \n",
    "            # We take the product of probabilities per individual per draw and then delete the ID column\n",
    "            P = P.groupby('ID', as_index=False).prod()\n",
    "            P = P.drop('ID', axis=1)\n",
    "            P['mean'] = P.mean(axis=1)\n",
    "            P = np.array(P['mean'])\n",
    "            \n",
    "    P = np.array(P)\n",
    "    ### --- This is where the panel data approach ends. --- ###\n",
    "    ############################################################################################################\n",
    "    \n",
    "    # We then take the log of the density function\n",
    "    logprob = np.log(P)\n",
    "    \n",
    "    return logprob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc1dcd",
   "metadata": {},
   "source": [
    "### Function 2: sum of log likelihood (SLL)\n",
    "This function simply takes the sum of log likelihood that we calculated with the first function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLL(betas):\n",
    "    return -sum(LL(betas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82189c90",
   "metadata": {},
   "source": [
    "## Model estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfe8f9",
   "metadata": {},
   "source": [
    "### Warnings\n",
    "\n",
    "Sometimes, optimisation procedures may 'overdo' it with warnings during estimation.\n",
    "We can supress these with the piece of code below (not always advisable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a142356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import t\n",
    "# with warnings.catch_warnings():\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84cc3d",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "\n",
    "Now it is finally time to run our estimation command.\n",
    "We use an optimisation algorith called 'BFGS'.\n",
    "\n",
    "**It is advisable not to edit the lines of code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ae316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give us the initial loglikelihood value as an output\n",
    "def callback1(betas):\n",
    "    print(\"Current log likelihood:\", -SLL(betas))\n",
    "\n",
    "# This function will allow as to store parameter estimates during iterations\n",
    "# Initialise list to store parameter values\n",
    "parameter_values = [np.array(list(betas_start.values()))]\n",
    "# Then define the function\n",
    "def callback2(betas):    \n",
    "    parameter_values.append(betas)\n",
    "    column_names = list(betas_start.keys())\n",
    "    with open(f'{model_name}_iterations.csv','w',newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(column_names)\n",
    "        writer.writerows(parameter_values)\n",
    "\n",
    "# Now let's combine the two callback functions\n",
    "def combined_callback(betas):\n",
    "    callback1(betas)\n",
    "    callback2(betas)\n",
    "        \n",
    "print(\"Initial log likelihood:\", -SLL(np.array(list(betas_start.values()))))\n",
    "\n",
    "# Choose optimisation routine (preferably BFGS)\n",
    "optimiser = 'BFGS' # BFGS or L-BFGS-B or nelder-mead\n",
    "\n",
    "result = minimize(SLL, np.array(list(betas_start.values())), method=optimiser,callback=combined_callback, \n",
    "                  options={'disp':False}) # ,bounds=bounds1\n",
    "#args = (parameter_values,)\n",
    "print(\"Final log likelihood:\", -result.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f05d5",
   "metadata": {},
   "source": [
    "## Post estimation processing\n",
    "\n",
    "We evaluate our parameter estimates using t-ratios (or p-Values).\n",
    "\n",
    "In maximum likelihood estimation, we extract these from the variance-covariance matrix of the parameters.\n",
    "\n",
    "The variance covariance matrix is not readily available but we need to calculate it.\n",
    "\n",
    "This is done with the code below.\n",
    "\n",
    "**DO NOT EDIT THE CHUNK OF CODE BELOW!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd351453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of parameter estimates\n",
    "parameters = result['x'] \n",
    "\n",
    "# Calculate hessian\n",
    "print(\"Calculating Hessian, please wait (this may take a while...)\")\n",
    "Hess = nd.Hessian(SLL,method='forward')\n",
    "hessian = Hess(parameters)\n",
    "inv_hessian = np.linalg.inv(hessian)\n",
    "\n",
    "# Parameter statistics\n",
    "dof = Nobs - len(betas_start) - 1\n",
    "se = np.sqrt(np.diag(inv_hessian)) # Standard errors\n",
    "tratio = parameters/se # t-ratios\n",
    "p_value = (1-t.cdf(np.abs(tratio),dof)) * 2 # p-values\n",
    "\n",
    "\n",
    "# --- Sandwich estimator --- #\n",
    "\n",
    "# The sandwich estimator provides the \"robust\" s.e., t-ratios and p-values.\n",
    "# These should be preferred over the classical parameter statistics.\n",
    "\n",
    "# We first need the gradients at the solution point\n",
    "Grad = nd.Gradient(LL,method='forward')\n",
    "gradient = Grad(parameters)\n",
    "\n",
    "# Then we need to calculate the B matrix\n",
    "B = np.array([])\n",
    "for r in range(gradient.shape[0]):\n",
    "    Bm = np.zeros([len(betas_start),len(betas_start)])\n",
    "    gradient0 = gradient[r,:]\n",
    "    for i in range(len(gradient0)):\n",
    "            for j in range(len(gradient0)):\n",
    "                element = gradient0[i]*gradient0[j]\n",
    "                Bm[i][j] = element\n",
    "    if B.size==0:\n",
    "                    B = Bm\n",
    "    else:\n",
    "                    B = B+Bm\n",
    "\n",
    "# Finally we \"sandwich\" the B matrix between the inverese hessian matrices\n",
    "BHHH = (inv_hessian)@(B)@(inv_hessian)\n",
    "\n",
    "print(\"... Done!!\")\n",
    "\n",
    "# Now it is time to calculate some \"robust\" parameter statistics\n",
    "rob_se = np.sqrt(np.diag(BHHH)) # robust standard error\n",
    "rob_tratio = parameters/rob_se # robust t-ratio\n",
    "rob_p_value = (1-t.cdf(np.abs(rob_tratio),dof)) * 2 # robust p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12586d23",
   "metadata": {},
   "source": [
    "## Results\n",
    "Finally, we got our results. Let's print them!\n",
    "\n",
    "The outputs that we receive are:\n",
    "* Estimates: These are the values of our parameters. We must check if the sign is consistent with our expectations\n",
    "* s.e.: Standard errors of the parameters\n",
    "* tratio: t-ratio of the parameters (significant if absolute value > 1.96)\n",
    "* p_value: p-value of the parameters (significant if < 0.05)\n",
    "\n",
    "The parameter statistics also have their **robust** versions. These should be preferred as they are less susceptible to mis-specified models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e411cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrays = np.column_stack((np.array(list(betas_start.keys())),parameters,se,tratio,p_value,rob_se,rob_tratio,rob_p_value))\n",
    "results = pd.DataFrame(arrays, columns = ['Parameter','Estimate','s.e.','t-ratio0','p-value',\n",
    "                                          'Rob s.e.','Rob t-ratio0','Rob p-value'])\n",
    "\n",
    "results[['Estimate','s.e.','t-ratio0','p-value','Rob s.e.','Rob t-ratio0','Rob p-value']] = (\n",
    "results[['Estimate','s.e.','t-ratio0','p-value','Rob s.e.','Rob t-ratio0','Rob p-value']].apply(pd.to_numeric, errors='coerce'))\n",
    "numeric_cols = results.select_dtypes(include='number').columns\n",
    "results[numeric_cols] = results[numeric_cols].round(3)\n",
    "results # print results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43ec05",
   "metadata": {},
   "source": [
    "## Goodness-of-fit indices\n",
    "\n",
    "Let's calculate some goodness-of-fit indices now (do not edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebbb79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First let's calculate the GoF indices\n",
    "\n",
    "rho_squared = 1 - ((-result.fun)/(-SLL(np.zeros(len(betas_start)))))\n",
    "adj_rho_squared = 1 - (((-result.fun)-len(betas_start))/(-SLL(np.zeros(len(betas_start)))))\n",
    "\n",
    "AIC = 2*len(betas_start) - 2*(-result.fun)\n",
    "BIC = len(betas_start)*np.log(Nobs) - 2*(-result.fun)\n",
    "\n",
    "LL0t = \"Log likelihood at zeros:\" + str(-SLL(np.zeros(len(betas_start))))\n",
    "LLinit = \"Initial log likelihood:\" + str(-SLL(np.array(list(betas_start.values()))))\n",
    "LLfin = \"Final log likelihood:\" + str(-result.fun)\n",
    "\n",
    "rs1 = \"rho squared=\"+str(rho_squared)\n",
    "rs2 = \"adjusted rho squared=\"+str(adj_rho_squared)\n",
    "ac = \"AIC=\"+str(AIC)\n",
    "bc = \"BIC=\"+str(BIC)\n",
    "\n",
    "# Then let's print the GoF indices\n",
    "\n",
    "print(LL0t)\n",
    "print(LLinit)\n",
    "print(LLfin)\n",
    "\n",
    "print(rs1)\n",
    "print(rs2)\n",
    "print(ac)\n",
    "print(bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c0bc6",
   "metadata": {},
   "source": [
    "## Save output\n",
    "\n",
    "We can save our output using the code below (do not edit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d92097",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{model_name}_results.txt\", 'w') as f:\n",
    "    f.write(f'{LL0t}\\n')\n",
    "    f.write(f'{LLinit}\\n')\n",
    "    f.write(f'{LLfin}\\n')\n",
    "    f.write(f'{rs1}\\n')\n",
    "    f.write(f'{rs2}\\n')\n",
    "    f.write(f'{ac}\\n')\n",
    "    f.write(f'{bc}\\n')\n",
    "    results.to_csv(f, index=False, sep='\\t')\n",
    "results.to_csv(f'{model_name}_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
