{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb37443",
   "metadata": {},
   "source": [
    "# Mixed logit model (MNL)\n",
    "\n",
    "This notebook was written by Evangelos Paschalidis (evangelos.paschalidis@epfl.ch) for the Decision-aid methodologies in transportation course at EPFL (http://edu.epfl.ch/coursebook/en/decision-aid-methodologies-in-transportation-CIVIL-557). \n",
    "\n",
    "Please contact before distributing or reusing the material below.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers the estimation of a mixed logit model in python with maximum likelihhod estimation:\n",
    "\n",
    "* Load necessary packages\n",
    "* Define variables and parameters to estimate\n",
    "* Model specification\n",
    "* Model output\n",
    "\n",
    "Have a go at working through the notebook. To run a code cell, just click on it (to see a green box around it) and then press the **Run** button at the top! \n",
    "\n",
    "Some cells have blank lines for you to complete. There is always a comment telling you what to do!\n",
    "\n",
    "You can also add a new cell by pressing the **+** button at the top of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07c2d1",
   "metadata": {},
   "source": [
    "## Context: Crowdshipping\n",
    "\n",
    "We have a database of crowdshippers acceptance or rejection of suggested routes. Each crowdshipper is evaluating based on variables such as parcel size, fragility, time of the day, travel time, compensation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec84370",
   "metadata": {},
   "source": [
    "## Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ad16e",
   "metadata": {},
   "source": [
    "## Load packages\n",
    "\n",
    "Before we estimate the model let's load some packages that we are going to need. When importing a package, it is common to rename it using an abbreviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a2baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data frame processing\n",
    "import numpy as np # for some statistical procedures\n",
    "from scipy.optimize import minimize # optimisation routine for parameter estimation\n",
    "from scipy.stats import norm # normal distribution density function\n",
    "import numdifftools as nd # we use this to calculate t-ratios and p-values\n",
    "from scipy.stats import t # we use this to calculate the CFDs needed for t-ratios and p-values\n",
    "import csv # we need this to store our parameters as csv\n",
    "\n",
    "# We've got some rookies in the mix.\n",
    "from scipy.special import roots_legendre # we use this to generate nodes for numerical integration\n",
    "from scipy.stats import qmc # we use this to generate nodes for numerical integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2660f834",
   "metadata": {},
   "source": [
    "### Let's give a name to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf4a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MNL_crowdshipping_mixed_solved' # Name we want to give to our model (this is used when saving the output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e295ca",
   "metadata": {},
   "source": [
    "### Panel structure\n",
    "We need to define whether our data is panel (i.e. multiple observations per individual) or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fb066cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = 0 # switch to 1 if data is panel (any other value if not panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f111947",
   "metadata": {},
   "source": [
    "### Define if we use mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d600289",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixing = 1 # switch to 1 if we apply mixing (any other value if no mixing applied)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12a2a2",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Now it is time to load the data. We can do that using the piece of code below.\n",
    "\n",
    "**Important!** Make sure the data is in the same folder with the notebook file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be01c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to load the data\n",
    "data = pd.read_table('data_crowdshipping.txt')\n",
    "\n",
    "# Number of observations (we need this to caclulate goodness-of-fit indices)\n",
    "Nobs = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e03b17",
   "metadata": {},
   "source": [
    "## Print the data\n",
    "\n",
    "Let's quickly print the data. Simply type *data* in the field below\n",
    "\n",
    "(If we open our data set with a different name e.g. *database* then the latter should be used)\n",
    "\n",
    "* Please make sure to change the name througout the whole script if you change the data frame name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "176b4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type \"data\" in this field (without the quotation) and run the cell (Shift + return)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcff136",
   "metadata": {},
   "source": [
    "## Print the variable names (columns)\n",
    "\n",
    "We can also print the variable names only using the piece of code below\n",
    "\n",
    "* This is useful during model specification to easily access the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e385d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'trip_duration', 'time_of_day', 'Size', 'Fragile', 'Compensation',\n",
      "       'choice'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32bad5f",
   "metadata": {},
   "source": [
    "## Variable definition\n",
    "\n",
    "We need to define the variables (as numpy arrays) that we will use in our model.\n",
    "\n",
    "* The arrays can have any name but it is more convenient to use the same name as in the data set.\n",
    "* The *\".reshape(-1, 1)\"* is not necessary now but it is required if we include mixing (random effects) in our model.\n",
    "    * Reshape allows to multiply arrays of different dimensions.\n",
    "\n",
    "##### Define explanatory (independent) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "353b6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example variable_name = np.array(data['variable_name']).reshape(-1, 1)\n",
    "\n",
    "choice = np.array(data['choice']).reshape(-1, 1)\n",
    "trip_duration = np.array(data['trip_duration']).reshape(-1, 1)\n",
    "time_of_day = np.array(data['time_of_day']).reshape(-1, 1)\n",
    "Size = np.array(data['Size']).reshape(-1, 1)\n",
    "Fragile = np.array(data['Fragile']).reshape(-1, 1)\n",
    "Compensation = np.array(data['Compensation']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429eb25",
   "metadata": {},
   "source": [
    "## Add new variables\n",
    "\n",
    "What if I want to add headway time for all alternatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d5835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "463f5c8a",
   "metadata": {},
   "source": [
    "##### Define the ID variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e31e65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example ID = np.array(data['ID']) # ID does not need to be reshaped\n",
    "\n",
    "ID = np.array(data['ID']) # ID does not need to be reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd30ab",
   "metadata": {},
   "source": [
    "##### Delete the data frame\n",
    "*We may want to delete the data frame to preserve some RAM (suggested for very large data sets)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a78ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate to delete the data frame.\n",
    "# del[data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58da27f",
   "metadata": {},
   "source": [
    "## Model specification\n",
    "\n",
    "We now need to create a function that includes our model.\n",
    "\n",
    "* Python functions are defined as: def *function_name*(parameters):\n",
    "* We end a function as: return *value_to_return*\n",
    "\n",
    "In the current implementation we specify two different functions as:\n",
    "* *function 1* calculates the log likelihood per observations\n",
    "* *function 2* calculates the sum of log likelihood taking as input the result from *function 1*\n",
    "\n",
    "*We define two different functions to be more flexible in the post estimation processing later in the code*\n",
    "\n",
    "We use some python (numpy) functions such '*exp*' or '*log*'. To execute these in the current example, we need to also call numpy; hence, we have *np.exp()* and *np.log()*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e2f8a5",
   "metadata": {},
   "source": [
    "### Define parameters and starting values\n",
    "\n",
    "Ultimately, we want to estimate the value of some parameters that maximise the likelihood of our observations of the dependent variable.\n",
    "\n",
    "Before starting the estimation process, we need to define some starting values for the parameters to be estimated. The typecal starting value is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ac51178",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_start = {\"asc\": 0, \"b_tt_mean\": 0, \"b_size\":0, \"b_fragile\":0, \"b_reward\":0,\n",
    "               \"b_morning\":0,\"b_afternoon\":0,\"b_evening\":0,\n",
    "                  \"sigma_tt\":0.1, \"sigma\":0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb4818",
   "metadata": {},
   "source": [
    "### Load old parameter estimates results\n",
    "\n",
    "Sometimes, we want to use results from old models as starting values.\n",
    "* To do that, we will load the iteration file from a previous estimation\n",
    "* Please note that only values of parameters with same names with our current model will be copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5641c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Activate this cell to load old results ###\n",
    "\n",
    "# # Open old iteration file\n",
    "# betas_old = pd.read_csv('model_name_iterations.csv') # file with old model iterations (edit accordingly)\n",
    "\n",
    "# # Keep last row\n",
    "# betas_old = betas_old.tail(1)\n",
    "\n",
    "# # Convert to dictionary\n",
    "# betas_old = betas_old.to_dict(orient='records')[0]\n",
    "\n",
    "# # Copy values from old to start for keys that are common to both dictionaries\n",
    "# betas_start = {k: betas_old[k] for k in betas_start.keys() & betas_old.keys()}\n",
    "\n",
    "# # Delete old estimates\n",
    "# del[betas_old]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a159696",
   "metadata": {},
   "source": [
    "#### Method 2: Code for simulation\n",
    "The simulation code will generate uniform random numbers (0, 1).\n",
    "\n",
    "These numbers must be then used in the model function (Function 1) to convert them to out target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "496f2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's set the number of draws\n",
    "# This indicates the number of random draws per random effect\n",
    "Ndraws = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1da03ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code is generating Halton draws using the number of nodes we set just above.\n",
    "# In this piece of code we define the names we want to use for... \n",
    "# ...the random numbers of the random effects.\n",
    "# We must make changes in the draws = ['draws_1','draws_2'] line of code only\n",
    "\n",
    "nIndiv = len(set(ID))\n",
    "draws = ['draws_tt','draws_sigma']\n",
    "nDim = len(draws)\n",
    "tasks = (pd.DataFrame(ID).value_counts(sort = False)) # observations per ID\n",
    "\n",
    "sampler = qmc.Halton(d=nDim, scramble=False)\n",
    "sample = pd.DataFrame(sampler.random(n=Ndraws*nIndiv+Ndraws)) # +Ndraws\n",
    "sample = sample[(Ndraws-1):(Ndraws*nIndiv+Ndraws-1)]\n",
    "\n",
    "cols = len(sample.columns)\n",
    "\n",
    "for i in range(cols):\n",
    "    # print(i)\n",
    "    sample1 = np.array(sample.loc[:,i])\n",
    "    sample1=pd.DataFrame(np.reshape(sample1,(nIndiv,Ndraws)))\n",
    "    sample_rep =sample1.loc[sample1.index.repeat(tasks)]\n",
    "    globals()[draws[i]] = sample_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abab6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to normal distribution (or any other preferred)\n",
    "# Here we use some funciton to convert the uniform draws we obtained\n",
    "# in the previous block of code to any distirbution\n",
    "# Example: draws_variable = norm.ppf(draws_variable)\n",
    "# The norm.ppf() command converts the uniform draws to standard normal\n",
    "\n",
    "draws_tt = norm.ppf(draws_tt)\n",
    "draws_sigma = norm.ppf(draws_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca7ff2",
   "metadata": {},
   "source": [
    "### Function 1: log likelihood (LL)\n",
    "This function calculates the log likelihood per observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "764b666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LL(betas): # betas is a vector with the parameters we want to estimate\n",
    "   \n",
    "    # First let's define the parameters to be estimated.\n",
    "    # The parameter names are imported directly from 'beta_start' that we defined earlier\n",
    "    \n",
    "    for pn in range(len(betas_start.values())):\n",
    "        globals()[np.array(list(betas_start.keys()))[pn]] = betas[pn]\n",
    "        \n",
    "    # Then we need to define the main model specification\n",
    "    \n",
    "    # We need to start by defining the utility functions\n",
    "    # Please make sure that you are using the same names for the parameters as those defined in 'betas_start'\n",
    "    \n",
    "    b_tt = b_tt_mean + sigma_tt*draws_tt\n",
    "    \n",
    "    U_accept = (asc + b_tt*trip_duration + \n",
    "            b_size*Size + b_fragile*Fragile + b_reward*Compensation +\n",
    "                draws_sigma*sigma +\n",
    "               b_morning*(time_of_day==1) + b_afternoon*(time_of_day==2) + b_evening*(time_of_day==3))\n",
    "    U_reject = 0\n",
    "    \n",
    "    # TASK ALERT!! How about adding the time of the day in the utility function?? #\n",
    "        # What is the variable type of time of the day? How many new parameters do we need to specify? #\n",
    "     \n",
    "    # And we need to take the exponents of utilities as\n",
    "    U_accept = np.exp(U_accept)\n",
    "    U_reject = np.exp(U_reject)\n",
    "    \n",
    "    # We need the sum of all utilities to calculate our choice probabilities\n",
    "    U_sum = U_accept + U_reject\n",
    "    \n",
    "    # And then we need to calculate our chouce probabilities\n",
    "    P_accept = U_accept / U_sum\n",
    "    P_reject = U_reject / U_sum\n",
    "    \n",
    "    # The total probability is then:\n",
    "    P = P_reject * (choice==0) + P_accept * (choice==1)\n",
    "    \n",
    "    ############################################################################################################\n",
    "    ############################################################################################################\n",
    "    # - Now this below is relevant if we have panel data and apply mixing (Do not change this piece of code!) -#\n",
    "    if panel == 1:\n",
    "    # Do it as panel\n",
    "        P = pd.DataFrame(P)\n",
    "        P = pd.concat([pd.Series(ID), pd.DataFrame(P)], axis=1, ignore_index=True)\n",
    "        P.rename(columns={P.columns[0]: 'ID'},inplace=True)\n",
    "    \n",
    "        # We take the product of probabilities per individual per draw and then delete the ID column\n",
    "        P = P.groupby('ID', as_index=False).prod()\n",
    "        P = P.drop('ID', axis=1)\n",
    "   \n",
    "    if mixing == 1:\n",
    "        # We take the average per row to get the average probability per individual (if mixing == 1)\n",
    "        \n",
    "        if panel == 1:\n",
    "            P['mean'] = P.mean(axis=1)\n",
    "            P = np.array(P['mean'])\n",
    "        \n",
    "        if panel == 0:\n",
    "            P = pd.DataFrame(P)\n",
    "            P = pd.concat([pd.Series(ID), pd.DataFrame(P)], axis=1, ignore_index=True)\n",
    "            P.rename(columns={P.columns[0]: 'ID'},inplace=True)\n",
    "    \n",
    "            # We take the product of probabilities per individual per draw and then delete the ID column\n",
    "            P = P.groupby('ID', as_index=False).prod()\n",
    "            P = P.drop('ID', axis=1)\n",
    "            P['mean'] = P.mean(axis=1)\n",
    "            P = np.array(P['mean'])\n",
    "            \n",
    "    P = np.array(P)\n",
    "    ### --- This is where the panel data approach ends. --- ###\n",
    "    ############################################################################################################\n",
    "    ############################################################################################################\n",
    "    \n",
    "    # We then take the log of the density function\n",
    "    logprob = np.log(P)\n",
    "    \n",
    "    return logprob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc1dcd",
   "metadata": {},
   "source": [
    "### Function 2: sum of log likelihood (SLL)\n",
    "This function simply takes the sum of log likelihood that we calculated with the first function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e74b0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLL(betas):\n",
    "    return -sum(LL(betas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82189c90",
   "metadata": {},
   "source": [
    "## Model estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfe8f9",
   "metadata": {},
   "source": [
    "### Warnings\n",
    "\n",
    "Sometimes, optimisation procedures may 'overdo' it with warnings during estimation.\n",
    "We can supress these with the piece of code below (not always advisable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a142356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84cc3d",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "\n",
    "Now it is finally time to run our estimation command.\n",
    "We use an optimisation algorith called 'BFGS'.\n",
    "\n",
    "**Do not to edit the lines of code below (except optimisation routine if needed).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f5ae316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial log likelihood: -6930.697502364465\n",
      "Current log likelihood: -6701.42692627947\n",
      "Current log likelihood: -6686.511743001411\n",
      "Current log likelihood: -6682.779291978648\n",
      "Current log likelihood: -6671.482092242548\n",
      "Current log likelihood: -6651.494381996537\n",
      "Current log likelihood: -6595.972458375579\n",
      "Current log likelihood: -6563.74598816578\n",
      "Current log likelihood: -6504.871483067333\n",
      "Current log likelihood: -6363.319537449224\n",
      "Current log likelihood: -6345.772192635529\n",
      "Current log likelihood: -6322.096118628811\n",
      "Current log likelihood: -6317.301391077239\n",
      "Current log likelihood: -6315.446325205706\n",
      "Current log likelihood: -6315.037718261939\n",
      "Current log likelihood: -6314.94043719116\n",
      "Current log likelihood: -6314.815148441102\n",
      "Current log likelihood: -6314.490109827133\n",
      "Current log likelihood: -6313.833354549357\n",
      "Current log likelihood: -6312.792262448648\n",
      "Current log likelihood: -6312.336438455155\n",
      "Current log likelihood: -6310.577434814391\n",
      "Current log likelihood: -6310.110605139542\n",
      "Current log likelihood: -6309.391617397538\n",
      "Current log likelihood: -6308.070523746533\n",
      "Current log likelihood: -6306.596843770023\n",
      "Current log likelihood: -6301.5172084327705\n",
      "Current log likelihood: -6291.394100787832\n",
      "Current log likelihood: -6274.162152660001\n",
      "Current log likelihood: -6273.560389376797\n",
      "Current log likelihood: -6261.466875046357\n",
      "Current log likelihood: -6258.169228742274\n",
      "Current log likelihood: -6257.9472796396885\n",
      "Current log likelihood: -6257.88896959392\n",
      "Current log likelihood: -6257.743214531902\n",
      "Current log likelihood: -6257.455661638318\n",
      "Current log likelihood: -6256.566382273355\n",
      "Current log likelihood: -6255.023561803568\n",
      "Current log likelihood: -6252.118277028681\n",
      "Current log likelihood: -6245.888717463826\n",
      "Current log likelihood: -6243.423412862033\n",
      "Current log likelihood: -6240.094957139772\n",
      "Current log likelihood: -6221.098530516792\n",
      "Current log likelihood: -6205.334952393484\n",
      "Current log likelihood: -6201.965174501336\n",
      "Current log likelihood: -6201.7066591375415\n",
      "Current log likelihood: -6201.115388203951\n",
      "Current log likelihood: -6200.543074111083\n",
      "Current log likelihood: -6200.335543971784\n",
      "Current log likelihood: -6200.215426529297\n",
      "Current log likelihood: -6200.165814829134\n",
      "Current log likelihood: -6199.926941505731\n",
      "Current log likelihood: -6199.624926257033\n",
      "Current log likelihood: -6198.651548837693\n",
      "Current log likelihood: -6196.993425009858\n",
      "Current log likelihood: -6193.045662956747\n",
      "Current log likelihood: -6191.137374535665\n",
      "Current log likelihood: -6186.776863631292\n",
      "Current log likelihood: -6185.715646878073\n",
      "Current log likelihood: -6185.222910751211\n",
      "Current log likelihood: -6184.969823607273\n",
      "Current log likelihood: -6184.739101676428\n",
      "Current log likelihood: -6184.454229753709\n",
      "Current log likelihood: -6184.287398955659\n",
      "Current log likelihood: -6183.897856464302\n",
      "Current log likelihood: -6183.669160435633\n",
      "Current log likelihood: -6183.544865784725\n",
      "Current log likelihood: -6183.491792092212\n",
      "Current log likelihood: -6183.38702002982\n",
      "Current log likelihood: -6183.3559411456135\n",
      "Current log likelihood: -6183.317362211974\n",
      "Current log likelihood: -6182.992204316204\n",
      "Current log likelihood: -6182.7167328369\n",
      "Current log likelihood: -6182.366415759731\n",
      "Current log likelihood: -6182.171061967894\n",
      "Current log likelihood: -6181.736201620567\n",
      "Current log likelihood: -6181.335780602717\n",
      "Current log likelihood: -6181.022105626254\n",
      "Current log likelihood: -6180.974361408288\n",
      "Current log likelihood: -6180.92106909822\n",
      "Current log likelihood: -6180.885968852605\n",
      "Current log likelihood: -6180.812030359343\n",
      "Current log likelihood: -6180.660594728669\n",
      "Current log likelihood: -6180.3863562894485\n",
      "Current log likelihood: -6179.879977293345\n",
      "Current log likelihood: -6179.297818478016\n",
      "Current log likelihood: -6179.008362902445\n",
      "Current log likelihood: -6178.932639044216\n",
      "Current log likelihood: -6178.907507349809\n",
      "Current log likelihood: -6178.899955511352\n",
      "Current log likelihood: -6178.887007635632\n",
      "Current log likelihood: -6178.8220502094855\n",
      "Current log likelihood: -6178.76865968321\n",
      "Current log likelihood: -6178.71022490922\n",
      "Current log likelihood: -6178.685515483632\n",
      "Current log likelihood: -6178.677856205368\n",
      "Current log likelihood: -6178.615685080004\n",
      "Current log likelihood: -6178.569106505123\n",
      "Current log likelihood: -6178.552644930064\n",
      "Current log likelihood: -6178.504033143287\n",
      "Current log likelihood: -6178.451121974219\n",
      "Current log likelihood: -6178.29822124222\n",
      "Current log likelihood: -6177.970819624669\n",
      "Current log likelihood: -6177.543231649844\n",
      "Current log likelihood: -6177.431454390956\n",
      "Current log likelihood: -6177.261158131147\n",
      "Current log likelihood: -6177.230174584123\n",
      "Current log likelihood: -6177.225736696307\n",
      "Current log likelihood: -6177.221430567094\n",
      "Current log likelihood: -6177.218858329824\n",
      "Current log likelihood: -6177.214581855347\n",
      "Current log likelihood: -6177.197704819781\n",
      "Current log likelihood: -6177.176793911946\n",
      "Current log likelihood: -6177.14229724535\n",
      "Current log likelihood: -6177.1169678839815\n",
      "Current log likelihood: -6177.0966339152965\n",
      "Current log likelihood: -6177.089409649266\n",
      "Current log likelihood: -6177.088498278317\n",
      "Current log likelihood: -6177.0874083465615\n",
      "Current log likelihood: -6177.087000331732\n",
      "Current log likelihood: -6177.08695534399\n",
      "Current log likelihood: -6177.086906070259\n",
      "Current log likelihood: -6177.086892122651\n",
      "Current log likelihood: -6177.086884362558\n",
      "Final log likelihood: -6177.086884362558\n"
     ]
    }
   ],
   "source": [
    "# This will give us the initial loglikelihood value as an output\n",
    "def callback1(betas):\n",
    "    print(\"Current log likelihood:\", -SLL(betas))\n",
    "\n",
    "# This function will allow as to store parameter estimates during iterations\n",
    "# Initialise list to store parameter values\n",
    "parameter_values = [np.array(list(betas_start.values()))]\n",
    "# Then define the function\n",
    "def callback2(betas):    \n",
    "    parameter_values.append(betas)\n",
    "    column_names = list(betas_start.keys())\n",
    "    with open(f'{model_name}_iterations.csv','w',newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(column_names)\n",
    "        writer.writerows(parameter_values)\n",
    "\n",
    "# Now let's combine the two callback functions\n",
    "def combined_callback(betas):\n",
    "    callback1(betas)\n",
    "    callback2(betas)\n",
    "        \n",
    "print(\"Initial log likelihood:\", -SLL(np.array(list(betas_start.values()))))\n",
    "\n",
    "# Choose optimisation routine (preferably BFGS)\n",
    "optimiser = 'L-BFGS-B' # BFGS or L-BFGS-B or nelder-mead\n",
    "\n",
    "result = minimize(SLL, np.array(list(betas_start.values())), method=optimiser,callback=combined_callback, \n",
    "                  options={'disp':False}) # ,bounds=bounds1\n",
    "#args = (parameter_values,)\n",
    "print(\"Final log likelihood:\", -result.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f05d5",
   "metadata": {},
   "source": [
    "## Post estimation processing\n",
    "\n",
    "We evaluate our parameter estimates using t-ratios (or p-Values).\n",
    "\n",
    "In maximum likelihood estimation, we extract these from the variance-covariance matrix of the parameters.\n",
    "\n",
    "The variance covariance matrix is not readily available but we need to calculate it.\n",
    "\n",
    "This is done with the code below.\n",
    "\n",
    "**DO NOT EDIT THE CHUNK OF CODE BELOW!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd351453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Hessian, please wait (this may take a while...)\n",
      "... Done!!\n"
     ]
    }
   ],
   "source": [
    "# Vector of parameter estimates\n",
    "parameters = result['x'] \n",
    "\n",
    "# Calculate hessian\n",
    "print(\"Calculating Hessian, please wait (this may take a while...)\")\n",
    "Hess = nd.Hessian(SLL)\n",
    "hessian = Hess(parameters)\n",
    "inv_hessian = np.linalg.inv(hessian)\n",
    "\n",
    "# Parameter statistics\n",
    "dof = Nobs - len(betas_start) - 1\n",
    "se = np.sqrt(np.diag(inv_hessian)) # Standard errors\n",
    "tratio = parameters/se # t-ratios\n",
    "p_value = (1-t.cdf(np.abs(tratio),dof)) * 2 # p-values\n",
    "\n",
    "\n",
    "# --- Sandwich estimator --- #\n",
    "\n",
    "# The sandwich estimator provides the \"robust\" s.e., t-ratios and p-values.\n",
    "# These should be preferred over the classical parameter statistics.\n",
    "\n",
    "# We first need the gradients at the solution point\n",
    "Grad = nd.Gradient(LL)\n",
    "gradient = Grad(parameters)\n",
    "\n",
    "# Then we need to calculate the B matrix\n",
    "B = np.array([])\n",
    "for r in range(gradient.shape[0]):\n",
    "    Bm = np.zeros([len(betas_start),len(betas_start)])\n",
    "    gradient0 = gradient[r,:]\n",
    "    for i in range(len(gradient0)):\n",
    "            for j in range(len(gradient0)):\n",
    "                element = gradient0[i]*gradient0[j]\n",
    "                Bm[i][j] = element\n",
    "    if B.size==0:\n",
    "                    B = Bm\n",
    "    else:\n",
    "                    B = B+Bm\n",
    "\n",
    "# Finally we \"sandwich\" the B matrix between the inverese hessian matrices\n",
    "BHHH = (inv_hessian)@(B)@(inv_hessian)\n",
    "\n",
    "print(\"... Done!!\")\n",
    "\n",
    "# Now it is time to calculate some \"robust\" parameter statistics\n",
    "rob_se = np.sqrt(np.diag(BHHH)) # robust standard error\n",
    "rob_tratio = parameters/rob_se # robust t-ratio\n",
    "rob_p_value = (1-t.cdf(np.abs(rob_tratio),dof)) * 2 # robust p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12586d23",
   "metadata": {},
   "source": [
    "## Results\n",
    "Finally, we got our results. Let's print them!\n",
    "\n",
    "The outputs that we receive are:\n",
    "* Estimates: These are the values of our parameters. We must check if the sign is consistent with our expectations\n",
    "* s.e.: Standard errors of the parameters\n",
    "* tratio: t-ratio of the parameters (significant if absolute value > 1.96)\n",
    "* p_value: p-value of the parameters (significant if < 0.05)\n",
    "\n",
    "The parameter statistics also have their **robust** versions. These should be preferred as they are less susceptible to weakly specified models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38e411cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Estimate</th>\n",
       "      <th>s.e.</th>\n",
       "      <th>t-ratio0</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Rob s.e.</th>\n",
       "      <th>Rob t-ratio0</th>\n",
       "      <th>Rob p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asc</td>\n",
       "      <td>-1.371</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-10.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-9.825</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b_tt_mean</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-1.749</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b_size</td>\n",
       "      <td>1.154</td>\n",
       "      <td>0.155</td>\n",
       "      <td>7.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>7.293</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b_fragile</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.060</td>\n",
       "      <td>16.622</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>15.965</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b_reward</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5.454</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5.337</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b_morning</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.074</td>\n",
       "      <td>9.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>8.968</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b_afternoon</td>\n",
       "      <td>1.381</td>\n",
       "      <td>0.088</td>\n",
       "      <td>15.744</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.093</td>\n",
       "      <td>14.891</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b_evening</td>\n",
       "      <td>1.087</td>\n",
       "      <td>0.080</td>\n",
       "      <td>13.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>13.016</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sigma_tt</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sigma</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1.330</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.963</td>\n",
       "      <td>1.471</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Parameter  Estimate   s.e.  t-ratio0  p-value  Rob s.e.  Rob t-ratio0  \\\n",
       "0          asc    -1.371  0.132   -10.399    0.000     0.140        -9.825   \n",
       "1    b_tt_mean    -0.004  0.002    -1.749    0.080     0.002        -1.743   \n",
       "2       b_size     1.154  0.155     7.447    0.000     0.158         7.293   \n",
       "3    b_fragile     0.995  0.060    16.622    0.000     0.062        15.965   \n",
       "4     b_reward     0.028  0.005     5.454    0.000     0.005         5.337   \n",
       "5    b_morning     0.669  0.074     9.094    0.000     0.075         8.968   \n",
       "6  b_afternoon     1.381  0.088    15.744    0.000     0.093        14.891   \n",
       "7    b_evening     1.087  0.080    13.600    0.000     0.084        13.016   \n",
       "8     sigma_tt     0.001  0.017     0.042    0.967     0.009         0.082   \n",
       "9        sigma     0.062  1.330     0.047    0.963     1.471         0.042   \n",
       "\n",
       "   Rob p-value  \n",
       "0        0.000  \n",
       "1        0.081  \n",
       "2        0.000  \n",
       "3        0.000  \n",
       "4        0.000  \n",
       "5        0.000  \n",
       "6        0.000  \n",
       "7        0.000  \n",
       "8        0.935  \n",
       "9        0.966  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrays = np.column_stack((np.array(list(betas_start.keys())),parameters,se,tratio,p_value,rob_se,rob_tratio,rob_p_value))\n",
    "results = pd.DataFrame(arrays, columns = ['Parameter','Estimate','s.e.','t-ratio0','p-value',\n",
    "                                          'Rob s.e.','Rob t-ratio0','Rob p-value'])\n",
    "\n",
    "results[['Estimate','s.e.','t-ratio0','p-value','Rob s.e.','Rob t-ratio0','Rob p-value']] = (\n",
    "results[['Estimate','s.e.','t-ratio0','p-value','Rob s.e.','Rob t-ratio0','Rob p-value']].apply(pd.to_numeric, errors='coerce'))\n",
    "numeric_cols = results.select_dtypes(include='number').columns\n",
    "results[numeric_cols] = results[numeric_cols].round(3)\n",
    "results # print results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679899b5",
   "metadata": {},
   "source": [
    "## Goodness-of-fit indices\n",
    "\n",
    "Let's calculate some goodness-of-fit indices now (do not edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6aeca001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood at zeros:-6931.471805600547\n",
      "Initial log likelihood:-6930.697502364465\n",
      "Final log likelihood:-6177.086884362558\n",
      "rho squared=0.10883473847912861\n",
      "adjusted rho squared=0.10739204343823994\n",
      "AIC=12374.173768725115\n",
      "BIC=12446.277172444878\n"
     ]
    }
   ],
   "source": [
    "# First let's calculate the GoF indices\n",
    "\n",
    "rho_squared = 1 - ((-result.fun)/(-SLL(np.zeros(len(betas_start)))))\n",
    "adj_rho_squared = 1 - (((-result.fun)-len(betas_start))/(-SLL(np.zeros(len(betas_start)))))\n",
    "\n",
    "AIC = 2*len(betas_start) - 2*(-result.fun)\n",
    "BIC = len(betas_start)*np.log(Nobs) - 2*(-result.fun)\n",
    "\n",
    "LL0t = \"Log likelihood at zeros:\" + str(-SLL(np.zeros(len(betas_start))))\n",
    "LLinit = \"Initial log likelihood:\" + str(-SLL(np.array(list(betas_start.values()))))\n",
    "LLfin = \"Final log likelihood:\" + str(-result.fun)\n",
    "\n",
    "rs1 = \"rho squared=\"+str(rho_squared)\n",
    "rs2 = \"adjusted rho squared=\"+str(adj_rho_squared)\n",
    "ac = \"AIC=\"+str(AIC)\n",
    "bc = \"BIC=\"+str(BIC)\n",
    "\n",
    "# Then let's print the GoF indices\n",
    "\n",
    "print(LL0t)\n",
    "print(LLinit)\n",
    "print(LLfin)\n",
    "\n",
    "print(rs1)\n",
    "print(rs2)\n",
    "print(ac)\n",
    "print(bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342cae7",
   "metadata": {},
   "source": [
    "## Save output\n",
    "\n",
    "We can save our output using the code below (do not edit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65b37d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{model_name}_results.txt\", 'w') as f:\n",
    "    f.write(f'{LL0t}\\n')\n",
    "    f.write(f'{LLinit}\\n')\n",
    "    f.write(f'{LLfin}\\n')\n",
    "    f.write(f'{rs1}\\n')\n",
    "    f.write(f'{rs2}\\n')\n",
    "    f.write(f'{ac}\\n')\n",
    "    f.write(f'{bc}\\n')\n",
    "    results.to_csv(f, index=False, sep='\\t')\n",
    "results.to_csv(f'{model_name}_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
